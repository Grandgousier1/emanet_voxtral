# üöÄ D√âPLOIEMENT RUNPOD JUPYTER - GUIDE COMPLET

## üìã D√©ploiement via Interface Jupyter RunPod

### **√âtape 1 : Pr√©paration du Pod**

```bash
# Dans le terminal Jupyter RunPod
cd /workspace

# Cloner le projet (ou upload via interface)
git clone https://github.com/votre-repo/emanet_voxtral.git
cd emanet_voxtral

# V√©rifier l'environnement
nvidia-smi
python --version
```

### **√âtape 2 : Installation Automatis√©e**

```bash
# Rendre le script ex√©cutable
chmod +x setup_runpod.sh

# Lancer l'installation compl√®te
./setup_runpod.sh
```

**Le script automatise :**
- ‚úÖ Installation des d√©pendances syst√®me
- ‚úÖ Configuration vLLM pour B200  
- ‚úÖ Installation Python packages optimis√©s
- ‚úÖ Pr√©-t√©l√©chargement des mod√®les Voxtral
- ‚úÖ Configuration variables d'environnement B200

### **√âtape 3 : Configuration Token HuggingFace**

#### **Option A : Interface guid√©e**
```bash
python quick_start.py
# S√©lectionner option 4 ‚Üí 1 pour configurer token
```

#### **Option B : Direct**
```bash
python main.py --setup-auth
```

#### **Option C : Manuel (dans notebook)**
```python
import os
import getpass

# Saisie s√©curis√©e du token
hf_token = getpass.getpass("üîë Token HuggingFace: ")

# Sauvegarde chiffr√©e
with open('.env', 'w') as f:
    f.write(f'HF_TOKEN="{hf_token}"\n')

print("‚úÖ Token configur√©")
```

### **√âtape 4 : Validation Environnement**

```bash
# Validation compl√®te B200
make validate-all-b200

# Ou validation rapide
python main.py --validate-only
```

### **√âtape 5 : Premier Test**

#### **Via Interface Guid√©e**
```bash
make start
# Suivre l'assistant interactif
```

#### **Via Jupyter Notebook**

```python
# Notebook cell 1: Import et configuration
import sys
sys.path.append('/workspace/emanet_voxtral')

from main import main
from cli_feedback import get_feedback
import argparse

# Configuration args
class Args:
    def __init__(self):
        self.url = "https://youtube.com/watch?v=VOTRE_VIDEO"
        self.output = "test_subtitle.srt"
        self.debug = True
        self.log_level = "INFO"
        self.use_voxtral_mini = False  # True pour mod√®le l√©ger
        self.force = False
        self.setup_auth = False
        self.validate_only = False
        self.dry_run = False

args = Args()
```

```python
# Notebook cell 2: Traitement
import os
os.chdir('/workspace/emanet_voxtral')

# Lance le traitement
result = main()
if result == 0:
    print("‚úÖ Traitement r√©ussi!")
    print(f"üìÑ Fichier g√©n√©r√©: {args.output}")
else:
    print("‚ùå Erreur de traitement")
```

### **√âtape 6 : Interface Web (Optionnel)**

```python
# Notebook cell: Lancer interface web simple
import gradio as gr
from pathlib import Path

def process_video(url, use_mini_model=False):
    """Interface Gradio pour traitement vid√©o"""
    try:
        from main import main
        import sys
        from types import SimpleNamespace
        
        # Cr√©er args
        args = SimpleNamespace(
            url=url,
            output=f"output_{int(time.time())}.srt",
            use_voxtral_mini=use_mini_model,
            debug=False,
            log_level="INFO",
            force=False,
            setup_auth=False,
            validate_only=False,
            dry_run=False
        )
        
        # Backup argv et remplacer
        original_argv = sys.argv
        sys.argv = ['main.py']
        
        try:
            # Traitement
            result = main()
            if result == 0:
                # Lire le fichier g√©n√©r√©
                with open(args.output, 'r', encoding='utf-8') as f:
                    content = f.read()
                return f"‚úÖ Succ√®s!\n\n{content}"
            else:
                return "‚ùå Erreur de traitement"
        finally:
            sys.argv = original_argv
            
    except Exception as e:
        return f"‚ùå Erreur: {e}"

# Interface Gradio
interface = gr.Interface(
    fn=process_video,
    inputs=[
        gr.Textbox(label="URL YouTube/Vid√©o", placeholder="https://youtube.com/watch?v=..."),
        gr.Checkbox(label="Utiliser Voxtral Mini (plus rapide)", value=False)
    ],
    outputs=gr.Textbox(label="Sous-titres g√©n√©r√©s", lines=20),
    title="üé¨ EMANET VOXTRAL - G√©n√©rateur de Sous-titres",
    description="Interface B200 optimis√©e pour g√©n√©ration de sous-titres"
)

# Lancer sur port 7860 (accessible via RunPod)
interface.launch(server_name="0.0.0.0", server_port=7860, share=True)
```

## üîß **Optimisations B200 Sp√©cifiques**

### **Variables d'environnement automatiques**
```bash
# Dans ~/.bashrc (configur√© par setup_runpod.sh)
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,expandable_segments:True
export CUDA_LAUNCH_BLOCKING=0
export TORCH_CUDA_ARCH_LIST=9.0
export VLLM_ATTENTION_BACKEND=FLASHINFER
export HF_HOME=/tmp/hf_cache
```

### **Monitoring en temps r√©el**
```python
# Notebook cell: Monitoring GPU
import time
import psutil
import torch
from IPython.display import clear_output

def monitor_resources():
    while True:
        clear_output(wait=True)
        
        # GPU Info
        if torch.cuda.is_available():
            gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
            gpu_used = torch.cuda.memory_allocated(0) / 1e9
            gpu_cached = torch.cuda.memory_reserved(0) / 1e9
            
            print(f"üéÆ GPU B200:")
            print(f"   M√©moire: {gpu_used:.1f}GB / {gpu_mem:.1f}GB ({gpu_used/gpu_mem*100:.1f}%)")
            print(f"   Cache: {gpu_cached:.1f}GB")
        
        # CPU/RAM
        cpu_percent = psutil.cpu_percent()
        ram = psutil.virtual_memory()
        
        print(f"\nüíª Syst√®me:")
        print(f"   CPU: {cpu_percent:.1f}%")
        print(f"   RAM: {ram.used/1e9:.1f}GB / {ram.total/1e9:.1f}GB ({ram.percent:.1f}%)")
        
        time.sleep(2)

# Lancer monitoring (Ctrl+C pour arr√™ter)
monitor_resources()
```

## üö® **Troubleshooting RunPod**

### **Probl√®mes courants**

#### **1. Erreur "CUDA out of memory"**
```python
# Notebook cell: Lib√©rer m√©moire GPU
import torch
import gc

torch.cuda.empty_cache()
gc.collect()
print("‚úÖ M√©moire GPU lib√©r√©e")
```

#### **2. Mod√®les non accessibles**
```bash
# V√©rifier token HF
python -c "import os; print('HF_TOKEN:', 'configur√©' if os.getenv('HF_TOKEN') else 'manquant')"

# Reconfigurer si n√©cessaire
python main.py --setup-auth
```

#### **3. Performance lente**
```python
# Notebook cell: Optimisations
import os

# Forcer optimisations B200
os.environ['VLLM_ATTENTION_BACKEND'] = 'FLASHINFER'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512,expandable_segments:True'

print("‚úÖ Optimisations B200 activ√©es")
```

## üìä **Validation D√©ploiement**

```bash
# Test complet du pipeline
make validate-all-b200

# Benchmark performance B200
python benchmark.py --b200-only

# Test avec vid√©o courte
python main.py --url "https://youtube.com/watch?v=SHORT_VIDEO" --output test.srt --debug
```

## üéØ **Points Cl√©s RunPod**

‚úÖ **Environment PyTorch 2.8.0 pr√©-install√©** - ne pas r√©installer  
‚úÖ **CUDA 12.8.1 optimis√© B200** - compatible natif  
‚úÖ **180GB VRAM** - peut traiter de tr√®s longues vid√©os  
‚úÖ **28 vCPU** - excellent pour preprocessing parall√®le  
‚úÖ **Interface Jupyter** - id√©al pour d√©veloppement/test  
‚úÖ **Persistance /workspace** - sauvegarde automatique  

Le projet est maintenant **PRODUCTION-READY** sur RunPod B200 ! üöÄ