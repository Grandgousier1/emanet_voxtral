#!/usr/bin/env python3
"""
cli_enhanced.py - Enhanced CLI with Interactive Setup and User Guidance
Comprehensive user-friendly CLI with step-by-step guidance and validation
"""

import os
import sys
import time
import json
import subprocess
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import getpass

from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.prompt import Prompt, Confirm, IntPrompt
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn
from rich.syntax import Syntax
from rich.text import Text
from rich.align import Align
from rich.columns import Columns
from rich.tree import Tree
from rich.status import Status
from rich.markdown import Markdown

console = Console()


class InteractiveCLI:
    """Enhanced interactive CLI for user-friendly experience."""
    
    def __init__(self):
        self.config_file = Path("cli_config.json")
        self.user_config = self.load_user_config()
        self.session_state = {
            "first_run": not self.config_file.exists(),
            "advanced_mode": False,
            "auto_mode": False
        }
    
    def load_user_config(self) -> Dict[str, Any]:
        """Load user configuration from file."""
        if self.config_file.exists():
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return {}
    
    def save_user_config(self):
        """Save user configuration to file."""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.user_config, f, indent=2, ensure_ascii=False)
        except Exception as e:
            console.print(f"[yellow]Impossible de sauvegarder la configuration: {e}[/yellow]")
    
    def display_welcome_banner(self):
        """Display enhanced welcome banner."""
        banner = """
[bold cyan]‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó[/bold cyan]
[bold cyan]‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë[/bold cyan]
[bold cyan]   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë        ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë[/bold cyan]
[bold cyan]   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë        ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë[/bold cyan]
[bold cyan]   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïë[/bold cyan]
[bold cyan]   ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù   ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïù[/bold cyan]
                                                                                      
[bold yellow]         üöÄ EMANET VOXTRAL - G√©n√©rateur de Sous-titres B200 üöÄ[/bold yellow]
        """
        
        info_text = """
[dim]Version 2.0 - Optimis√© pour GPU B200 (180GB VRAM)[/dim]
[dim]Traduction haute qualit√© avec mod√®les Voxtral de Mistral AI[/dim]

‚ú® [bold]Fonctionnalit√©s principales :[/bold]
   ‚Ä¢ Traitement vid√©o/audio automatique
   ‚Ä¢ G√©n√©ration de sous-titres multilingues
   ‚Ä¢ Optimisation GPU B200 avanc√©e
   ‚Ä¢ Interface interactive guid√©e
   ‚Ä¢ Monitoring temps r√©el
        """
        
        console.print(Panel.fit(
            f"{banner}\n{info_text}",
            border_style="cyan",
            padding=(1, 2)
        ))
    
    def show_quick_start_menu(self) -> str:
        """Show quick start menu for new users."""
        if not self.session_state["first_run"] and "preferred_mode" in self.user_config:
            return self.user_config["preferred_mode"]
        
        console.print("\n[bold]üéØ Comment souhaitez-vous commencer ?[/bold]\n")
        
        options = {
            "1": ("üé¨ Mode Simple", "Traiter une vid√©o/audio unique avec assistance"),
            "2": ("üì¶ Mode Batch", "Traiter plusieurs fichiers en lot"),
            "3": ("‚öôÔ∏è  Configuration", "Configurer l'environnement et les pr√©f√©rences"),
            "4": ("üîß Mode Avanc√©", "Options avanc√©es pour utilisateurs exp√©riment√©s"),
            "5": ("üìö Guide d'utilisation", "Tutoriel complet pas √† pas"),
            "6": ("üè• Diagnostic", "V√©rifier l'√©tat du syst√®me")
        }
        
        for key, (title, desc) in options.items():
            console.print(f"  [bold cyan]{key}[/bold cyan]. {title}")
            console.print(f"     [dim]{desc}[/dim]")
        
        choice = Prompt.ask(
            "\n[bold]Votre choix",
            choices=list(options.keys()),
            default="1"
        )
        
        mode_map = {
            "1": "simple",
            "2": "batch", 
            "3": "config",
            "4": "advanced",
            "5": "tutorial",
            "6": "diagnostic"
        }
        
        selected_mode = mode_map[choice]
        
        # Save preference for returning users
        if self.session_state["first_run"]:
            save_pref = Confirm.ask("üíæ Sauvegarder ce choix comme pr√©f√©rence par d√©faut ?")
            if save_pref:
                self.user_config["preferred_mode"] = selected_mode
                self.save_user_config()
        
        return selected_mode
    
    def run_system_diagnostic(self):
        """Run comprehensive system diagnostic with fixes."""
        console.print(Panel("üè• [bold]Diagnostic Syst√®me Complet[/bold]", style="blue"))
        
        checks = [
            ("üîß D√©pendances Python", self._check_dependencies),
            ("üéÆ Configuration GPU", self._check_gpu_setup),
            ("üíæ Espace disque", self._check_disk_space),
            ("üîë Token Hugging Face", self._check_hf_token),
            ("üìÅ Structure projet", self._check_project_structure),
            ("üöÄ Mod√®les disponibles", self._check_models),
            ("‚ö° Performance syst√®me", self._check_performance)
        ]
        
        results = []
        with Progress(SpinnerColumn(), TextColumn("{task.description}"), transient=True) as progress:
            for name, check_func in checks:
                task = progress.add_task(f"V√©rification: {name}", total=1)
                result = check_func()
                results.append((name, result))
                progress.update(task, completed=1)
                time.sleep(0.5)  # Visual feedback
        
        self._display_diagnostic_results(results)
        self._offer_fixes(results)
    
    def _check_dependencies(self) -> Dict[str, Any]:
        """Check Python dependencies."""
        try:
            import torch
            import rich
            import soundfile
            return {
                "status": "success",
                "message": "Toutes les d√©pendances critiques sont install√©es",
                "details": f"PyTorch: {torch.__version__}, Rich: {rich.__version__}"
            }
        except ImportError as e:
            return {
                "status": "error",
                "message": f"D√©pendance manquante: {e}",
                "fix": "pip install -r requirements.txt"
            }
    
    def _check_gpu_setup(self) -> Dict[str, Any]:
        """Check GPU configuration."""
        try:
            import torch
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                gpu_names = [torch.cuda.get_device_name(i) for i in range(gpu_count)]
                memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                
                return {
                    "status": "success",
                    "message": f"{gpu_count} GPU(s) d√©tect√©(s)",
                    "details": f"GPU(s): {', '.join(gpu_names)}, M√©moire: {memory_gb:.1f}GB"
                }
            else:
                return {
                    "status": "warning",
                    "message": "Aucun GPU CUDA d√©tect√©",
                    "fix": "V√©rifier l'installation CUDA/pilotes"
                }
        except Exception as e:
            return {
                "status": "error",
                "message": f"Erreur GPU: {e}",
                "fix": "R√©installer PyTorch avec support CUDA"
            }
    
    def _check_disk_space(self) -> Dict[str, Any]:
        """Check available disk space."""
        import shutil
        total, used, free = shutil.disk_usage('.')
        free_gb = free / (1024**3)
        
        if free_gb > 25:
            return {
                "status": "success",
                "message": f"Espace suffisant: {free_gb:.1f}GB libres"
            }
        elif free_gb > 10:
            return {
                "status": "warning",
                "message": f"Espace limit√©: {free_gb:.1f}GB libres",
                "fix": "Lib√©rer de l'espace disque (recommand√©: >25GB)"
            }
        else:
            return {
                "status": "error",
                "message": f"Espace insuffisant: {free_gb:.1f}GB libres",
                "fix": "Lib√©rer au moins 25GB d'espace disque"
            }
    
    def _check_hf_token(self) -> Dict[str, Any]:
        """Check Hugging Face token."""
        token = os.getenv('HF_TOKEN') or os.getenv('HUGGING_FACE_HUB_TOKEN')
        if token:
            return {
                "status": "success",
                "message": "Token Hugging Face configur√©"
            }
        else:
            return {
                "status": "error",
                "message": "Token Hugging Face manquant",
                "fix": "Configurer HF_TOKEN dans les variables d'environnement"
            }
    
    def _check_project_structure(self) -> Dict[str, Any]:
        """Check project file structure."""
        required_files = [
            "main.py", "config.py", "parallel_processor.py",
            "utils/model_utils.py", "utils/gpu_utils.py"
        ]
        
        missing = [f for f in required_files if not Path(f).exists()]
        
        if not missing:
            return {
                "status": "success",
                "message": "Structure projet compl√®te"
            }
        else:
            return {
                "status": "error",
                "message": f"Fichiers manquants: {', '.join(missing)}",
                "fix": "T√©l√©charger le projet complet depuis le repository"
            }
    
    def _check_models(self) -> Dict[str, Any]:
        """Check available models."""
        # This would check if models are cached/available
        return {
            "status": "info",
            "message": "Mod√®les seront t√©l√©charg√©s √† la premi√®re utilisation",
            "details": "Voxtral-Small-24B, Voxtral-Mini-3B"
        }
    
    def _check_performance(self) -> Dict[str, Any]:
        """Check system performance indicators."""
        import psutil
        
        cpu_count = psutil.cpu_count()
        memory_gb = psutil.virtual_memory().total / (1024**3)
        
        return {
            "status": "info",
            "message": f"Syst√®me: {cpu_count} CPU, {memory_gb:.1f}GB RAM",
            "details": "Performance optimale avec 28+ vCPU, 188+ GB RAM"
        }
    
    def _display_diagnostic_results(self, results: List[Tuple[str, Dict]]):
        """Display diagnostic results in a table."""
        table = Table(title="üìä R√©sultats du Diagnostic", show_header=True, header_style="bold magenta")
        table.add_column("Composant", style="cyan", width=20)
        table.add_column("Statut", justify="center", width=10)
        table.add_column("Message", width=40)
        table.add_column("D√©tails", style="dim", width=30)
        
        for name, result in results:
            status = result["status"]
            if status == "success":
                status_icon = "[green]‚úÖ OK[/green]"
            elif status == "warning":
                status_icon = "[yellow]‚ö†Ô∏è  WARN[/yellow]"
            elif status == "error":
                status_icon = "[red]‚ùå ERR[/red]"
            else:
                status_icon = "[blue]‚ÑπÔ∏è  INFO[/blue]"
            
            table.add_row(
                name,
                status_icon,
                result["message"],
                result.get("details", "")
            )
        
        console.print(table)
    
    def _offer_fixes(self, results: List[Tuple[str, Dict]]):
        """Offer to fix detected issues."""
        issues = [(name, result) for name, result in results if result["status"] in ["error", "warning"] and "fix" in result]
        
        if not issues:
            console.print("\n[green]‚ú® Aucun probl√®me d√©tect√© ! Votre syst√®me est pr√™t.[/green]")
            return
        
        console.print(f"\n[yellow]üîß {len(issues)} probl√®me(s) d√©tect√©(s) avec des solutions disponibles:[/yellow]")
        
        for name, result in issues:
            console.print(f"\n[bold]{name}:[/bold]")
            console.print(f"  Probl√®me: {result['message']}")
            console.print(f"  Solution: [cyan]{result['fix']}[/cyan]")
            
            if Confirm.ask(f"  Voulez-vous que je tente de r√©soudre ce probl√®me automatiquement ?"):
                self._auto_fix_issue(name, result)
    
    def _auto_fix_issue(self, name: str, result: Dict):
        """Attempt to automatically fix an issue."""
        fix_command = result.get("fix", "")
        
        if "pip install" in fix_command:
            console.print("üîÑ Installation des d√©pendances...")
            try:
                subprocess.run(fix_command.split(), check=True, capture_output=True)
                console.print("[green]‚úÖ D√©pendances install√©es avec succ√®s![/green]")
            except subprocess.CalledProcessError as e:
                console.print(f"[red]‚ùå √âchec de l'installation: {e}[/red]")
        
        elif "HF_TOKEN" in fix_command:
            console.print("üîë Configuration du token Hugging Face...")
            self.setup_hf_token_interactive()
        
        else:
            console.print(f"[yellow]Solution manuelle requise: {fix_command}[/yellow]")
    
    def setup_hf_token_interactive(self):
        """Interactive Hugging Face token setup."""
        console.print(Panel("""
[bold]Configuration du Token Hugging Face[/bold]

Pour utiliser les mod√®les Voxtral, vous devez:
1. Cr√©er un compte sur https://huggingface.co
2. Aller dans Settings ‚Üí Access Tokens
3. Cr√©er un nouveau token avec permissions de lecture
4. Copier le token ici

[dim]Le token ne sera pas affich√© pendant la saisie pour des raisons de s√©curit√©.[/dim]
        """, style="blue"))
        
        if Confirm.ask("Avez-vous d√©j√† un token Hugging Face ?"):
            token = getpass.getpass("Entrez votre token HF (invisible): ")
            
            if token.strip():
                # Save to .env file with encryption for security
                env_file = Path(".env")
                try:
                    from utils.auth_manager import TokenManager
                    token_manager = TokenManager()
                    encrypted_token = token_manager._encrypt_token(token.strip())
                    
                    with open(env_file, "a") as f:
                        f.write(f"\nHF_TOKEN={encrypted_token}\n")
                    
                    console.print("[green]‚úÖ Token chiffr√© et sauvegard√© dans .env[/green]")
                except ImportError:
                    # Fallback to plain text if encryption not available
                    with open(env_file, "a") as f:
                        f.write(f"\nHF_TOKEN={token.strip()}\n")
                    
                    console.print("[green]‚úÖ Token sauvegard√© dans .env[/green]")
                    console.print("[yellow]‚ö†Ô∏è  Installez 'cryptography' pour un stockage s√©curis√©[/yellow]")
                console.print("[yellow]‚ÑπÔ∏è  Red√©marrez l'application pour prendre en compte le token[/yellow]")
            else:
                console.print("[red]‚ùå Token vide, configuration annul√©e[/red]")
        else:
            console.print("Visitez https://huggingface.co pour cr√©er un compte et obtenir un token.")
    
    def show_tutorial(self):
        """Show comprehensive tutorial."""
        console.print(Panel("üìö [bold]Guide d'Utilisation Complet[/bold]", style="green"))
        
        sections = [
            ("üéØ Vue d'ensemble", self._tutorial_overview),
            ("‚öôÔ∏è Configuration initiale", self._tutorial_setup),
            ("üé¨ Traitement simple", self._tutorial_simple),
            ("üì¶ Traitement en lot", self._tutorial_batch),
            ("üîß Options avanc√©es", self._tutorial_advanced),
            ("üö® R√©solution de probl√®mes", self._tutorial_troubleshooting)
        ]
        
        for title, content_func in sections:
            if Confirm.ask(f"\nAfficher: {title} ?", default=True):
                content_func()
                if not Confirm.ask("Continuer vers la section suivante ?", default=True):
                    break
    
    def _tutorial_overview(self):
        """Tutorial overview section."""
        content = """
[bold]EMANET VOXTRAL - Vue d'ensemble[/bold]

Ce logiciel transforme automatiquement vos vid√©os/audios en sous-titres traduits de haute qualit√©.

[bold cyan]Processus typique :[/bold cyan]
1. üì• T√©l√©chargement/chargement du m√©dia (YouTube, fichier local)
2. üéµ Extraction et traitement de l'audio
3. üó£Ô∏è  D√©tection des segments de parole (VAD)
4. ü§ñ Transcription avec mod√®les Voxtral (Mistral AI)
5. üåç Traduction vers la langue cible
6. üíæ G√©n√©ration du fichier SRT

[bold yellow]Formats support√©s :[/bold yellow]
‚Ä¢ Entr√©e: YouTube, MP4, MP3, WAV, M4A, et plus
‚Ä¢ Sortie: Fichiers SRT (sous-titres)

[bold green]Optimisations B200 :[/bold green]
‚Ä¢ Traitement GPU parall√®le haute performance
‚Ä¢ Gestion intelligente de la m√©moire
‚Ä¢ Monitoring temps r√©el des ressources
        """
        console.print(Panel(content, title="Vue d'ensemble", border_style="cyan"))
    
    def _tutorial_setup(self):
        """Tutorial setup section."""
        content = """
[bold]Configuration Initiale[/bold]

[bold cyan]1. Token Hugging Face (OBLIGATOIRE)[/bold cyan]
   ‚Ä¢ Cr√©er un compte sur https://huggingface.co
   ‚Ä¢ G√©n√©rer un token d'acc√®s
   ‚Ä¢ Le configurer via l'option 'Configuration'

[bold cyan]2. V√©rification GPU[/bold cyan]
   ‚Ä¢ GPU B200 recommand√© (180GB VRAM)
   ‚Ä¢ Pilotes CUDA √† jour
   ‚Ä¢ PyTorch avec support GPU

[bold cyan]3. Espace disque[/bold cyan]
   ‚Ä¢ Minimum 25GB libres
   ‚Ä¢ Plus pour le traitement en lot

[bold cyan]4. Variables d'environnement[/bold cyan]
   ‚Ä¢ HF_TOKEN=votre_token_huggingface
   ‚Ä¢ Optionnel: CUDA_VISIBLE_DEVICES=0
        """
        console.print(Panel(content, title="Configuration", border_style="green"))
    
    def _tutorial_simple(self):
        """Tutorial simple processing section."""
        content = """
[bold]Traitement Simple - Mode Guid√©[/bold]

[bold cyan]Commande de base :[/bold cyan]
python main.py --url "https://youtube.com/watch?v=..." --output sous_titres.srt

[bold cyan]Options courantes :[/bold cyan]
‚Ä¢ --target-lang fr          : Langue de traduction (fr, en, es, de, it...)
‚Ä¢ --model voxtral-small     : Mod√®le √† utiliser (small/mini)
‚Ä¢ --quality best            : Qualit√© (fast/balanced/best)
‚Ä¢ --debug                   : Mode debug pour plus d'informations

[bold cyan]Exemple complet :[/bold cyan]
python main.py \\
  --url "https://youtube.com/watch?v=dQw4w9WgXcQ" \\
  --output "rick_roll_fr.srt" \\
  --target-lang fr \\
  --quality best

[bold yellow]Le syst√®me vous guidera √† chaque √©tape ![/bold yellow]
        """
        console.print(Panel(content, title="Mode Simple", border_style="blue"))
    
    def _tutorial_batch(self):
        """Tutorial batch processing section."""
        content = """
[bold]Traitement en Lot[/bold]

[bold cyan]1. Cr√©er un fichier de lot (batch.txt) :[/bold cyan]
https://youtube.com/watch?v=video1
https://youtube.com/watch?v=video2
/chemin/vers/fichier_local.mp4

[bold cyan]2. Lancer le traitement :[/bold cyan]
python main.py --batch-list batch.txt --output-dir resultats/

[bold cyan]3. Structure de sortie :[/bold cyan]
resultats/
‚îú‚îÄ‚îÄ video1_fr.srt
‚îú‚îÄ‚îÄ video2_fr.srt
‚îî‚îÄ‚îÄ fichier_local_fr.srt

[bold cyan]Options avanc√©es :[/bold cyan]
‚Ä¢ --max-workers 4           : Nombre de workers parall√®les
‚Ä¢ --skip-existing           : Ignorer les fichiers d√©j√† trait√©s
‚Ä¢ --continue-on-error       : Continuer m√™me en cas d'erreur
        """
        console.print(Panel(content, title="Mode Batch", border_style="yellow"))
    
    def _tutorial_advanced(self):
        """Tutorial advanced options section."""
        content = """
[bold]Options Avanc√©es[/bold]

[bold cyan]Performance GPU :[/bold cyan]
‚Ä¢ --gpu-memory-limit 0.9    : Limite m√©moire GPU (0.1-0.95)
‚Ä¢ --batch-size 32           : Taille des lots de traitement
‚Ä¢ --precision bf16          : Pr√©cision (fp16/bf16/fp32)

[bold cyan]Param√®tres audio :[/bold cyan]
‚Ä¢ --vad-threshold 0.3       : Seuil de d√©tection vocale
‚Ä¢ --min-segment-duration 1.0: Dur√©e minimale des segments
‚Ä¢ --audio-quality high      : Qualit√© audio (low/medium/high)

[bold cyan]Contr√¥le de qualit√© :[/bold cyan]
‚Ä¢ --min-quality-score 0.7   : Score qualit√© minimal
‚Ä¢ --retry-failed-segments   : R√©essayer les segments √©chou√©s
‚Ä¢ --quality-check           : Validation qualit√© post-traitement

[bold cyan]Monitoring :[/bold cyan]
‚Ä¢ --monitor                 : Interface monitoring temps r√©el
‚Ä¢ --telemetry               : Collecte de m√©triques avanc√©es
        """
        console.print(Panel(content, title="Options Avanc√©es", border_style="magenta"))
    
    def _tutorial_troubleshooting(self):
        """Tutorial troubleshooting section."""
        content = """
[bold]R√©solution de Probl√®mes[/bold]

[bold red]Erreurs courantes :[/bold red]

[bold cyan]1. "CUDA out of memory"[/bold cyan]
   ‚Üí R√©duire --batch-size ou --gpu-memory-limit
   ‚Üí Fermer autres applications GPU

[bold cyan]2. "Token HF invalide"[/bold cyan]
   ‚Üí V√©rifier le token dans .env
   ‚Üí R√©g√©n√©rer un nouveau token sur HF

[bold cyan]3. "Espace disque insuffisant"[/bold cyan]
   ‚Üí Lib√©rer au moins 25GB
   ‚Üí Nettoyer le cache: rm -rf ~/.cache/huggingface/

[bold cyan]4. "Erreur de t√©l√©chargement YouTube"[/bold cyan]
   ‚Üí V√©rifier l'URL
   ‚Üí Mettre √† jour yt-dlp: pip install -U yt-dlp

[bold yellow]Outils de diagnostic :[/bold yellow]
‚Ä¢ python main.py --validate  : Validation compl√®te
‚Ä¢ python validator.py        : Tests d√©taill√©s
‚Ä¢ Option 6 du menu principal : Diagnostic interactif

[bold green]Support :[/bold green]
‚Ä¢ Mode --debug pour plus d'infos
‚Ä¢ Logs d√©taill√©s dans emanet.log
‚Ä¢ Monitoring temps r√©el avec --monitor
        """
        console.print(Panel(content, title="D√©pannage", border_style="red"))
    
    def configure_system_interactive(self):
        """Interactive system configuration."""
        console.print(Panel("‚öôÔ∏è [bold]Configuration Syst√®me Interactive[/bold]", style="blue"))
        
        config_options = [
            ("üîë Token Hugging Face", self.setup_hf_token_interactive),
            ("üéÆ Configuration GPU", self._configure_gpu),
            ("üìÅ R√©pertoires de travail", self._configure_directories),
            ("üåç Langue par d√©faut", self._configure_default_language),
            ("‚ö° Param√®tres performance", self._configure_performance),
            ("üìä Monitoring et logs", self._configure_monitoring),
            ("üíæ Sauvegarder configuration", self._save_configuration)
        ]
        
        while True:
            console.print("\n[bold]Options de configuration :[/bold]")
            for i, (name, _) in enumerate(config_options, 1):
                console.print(f"  {i}. {name}")
            console.print("  0. Retour au menu principal")
            
            choice = IntPrompt.ask("Votre choix", choices=[str(i) for i in range(len(config_options) + 1)])
            
            if choice == 0:
                break
            elif 1 <= choice <= len(config_options):
                config_options[choice - 1][1]()
    
    def _configure_gpu(self):
        """Configure GPU settings."""
        console.print("üéÆ [bold]Configuration GPU[/bold]")
        
        # Auto-detect GPU info
        try:
            import torch
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                for i in range(gpu_count):
                    props = torch.cuda.get_device_properties(i)
                    console.print(f"GPU {i}: {props.name} ({props.total_memory / 1024**3:.1f}GB)")
                
                # Memory limit configuration
                current_limit = self.user_config.get("gpu_memory_limit", 0.9)
                new_limit = Prompt.ask(
                    f"Limite m√©moire GPU (0.1-0.95)",
                    default=str(current_limit)
                )
                
                try:
                    self.user_config["gpu_memory_limit"] = float(new_limit)
                    console.print(f"[green]‚úÖ Limite m√©moire GPU: {new_limit}[/green]")
                except ValueError:
                    console.print("[red]‚ùå Valeur invalide[/red]")
                
                # Batch size configuration
                current_batch = self.user_config.get("batch_size", 32)
                new_batch = IntPrompt.ask(
                    "Taille de lot par d√©faut",
                    default=current_batch
                )
                self.user_config["batch_size"] = new_batch
                
            else:
                console.print("[yellow]‚ö†Ô∏è Aucun GPU CUDA d√©tect√©[/yellow]")
                
        except ImportError:
            console.print("[red]‚ùå PyTorch non disponible[/red]")
    
    def _configure_directories(self):
        """Configure working directories."""
        console.print("üìÅ [bold]Configuration des R√©pertoires[/bold]")
        
        # Output directory
        current_output = self.user_config.get("default_output_dir", "./output")
        new_output = Prompt.ask("R√©pertoire de sortie par d√©faut", default=current_output)
        
        output_path = Path(new_output)
        output_path.mkdir(parents=True, exist_ok=True)
        self.user_config["default_output_dir"] = str(output_path)
        
        # Cache directory
        current_cache = self.user_config.get("cache_dir", "./cache")
        new_cache = Prompt.ask("R√©pertoire cache", default=current_cache)
        
        cache_path = Path(new_cache)
        cache_path.mkdir(parents=True, exist_ok=True)
        self.user_config["cache_dir"] = str(cache_path)
        
        console.print("[green]‚úÖ R√©pertoires configur√©s[/green]")
    
    def _configure_default_language(self):
        """Configure default target language."""
        languages = {
            "fr": "Fran√ßais",
            "en": "English", 
            "es": "Espa√±ol",
            "de": "Deutsch",
            "it": "Italiano",
            "pt": "Portugu√™s",
            "ru": "–†—É—Å—Å–∫–∏–π",
            "zh": "‰∏≠Êñá",
            "ja": "Êó•Êú¨Ë™û",
            "ar": "ÿßŸÑÿπÿ±ÿ®Ÿäÿ©"
        }
        
        console.print("üåç [bold]Langue de traduction par d√©faut[/bold]")
        for code, name in languages.items():
            console.print(f"  {code}: {name}")
        
        current_lang = self.user_config.get("default_target_language", "fr")
        new_lang = Prompt.ask(
            "Code langue",
            choices=list(languages.keys()),
            default=current_lang
        )
        
        self.user_config["default_target_language"] = new_lang
        console.print(f"[green]‚úÖ Langue par d√©faut: {languages[new_lang]}[/green]")
    
    def _configure_performance(self):
        """Configure performance settings."""
        console.print("‚ö° [bold]Param√®tres de Performance[/bold]")
        
        # Quality preset
        quality_presets = ["fast", "balanced", "best"]
        current_quality = self.user_config.get("default_quality", "balanced")
        new_quality = Prompt.ask(
            "Pr√©r√©glage qualit√© par d√©faut",
            choices=quality_presets,
            default=current_quality
        )
        self.user_config["default_quality"] = new_quality
        
        # Parallel workers
        import psutil
        max_workers = psutil.cpu_count()
        current_workers = self.user_config.get("max_workers", min(4, max_workers))
        new_workers = IntPrompt.ask(
            f"Nombre de workers parall√®les (1-{max_workers})",
            default=current_workers
        )
        if 1 <= new_workers <= max_workers:
            self.user_config["max_workers"] = new_workers
        
        console.print("[green]‚úÖ Param√®tres de performance mis √† jour[/green]")
    
    def _configure_monitoring(self):
        """Configure monitoring and logging."""
        console.print("üìä [bold]Configuration Monitoring[/bold]")
        
        # Enable monitoring by default
        enable_monitoring = Confirm.ask("Activer le monitoring temps r√©el ?", default=True)
        self.user_config["enable_monitoring"] = enable_monitoring
        
        # Telemetry
        enable_telemetry = Confirm.ask("Activer la t√©l√©m√©trie (m√©triques avanc√©es) ?", default=True)
        self.user_config["enable_telemetry"] = enable_telemetry
        
        # Log level
        log_levels = ["INFO", "DEBUG", "WARNING", "ERROR"]
        current_level = self.user_config.get("log_level", "INFO")
        new_level = Prompt.ask(
            "Niveau de logs",
            choices=log_levels,
            default=current_level
        )
        self.user_config["log_level"] = new_level
        
        console.print("[green]‚úÖ Configuration monitoring mise √† jour[/green]")
    
    def _save_configuration(self):
        """Save current configuration."""
        self.save_user_config()
        console.print("[green]‚úÖ Configuration sauvegard√©e dans cli_config.json[/green]")
        
        # Display current config summary
        if self.user_config:
            table = Table(title="Configuration Actuelle", show_header=True)
            table.add_column("Param√®tre", style="cyan")
            table.add_column("Valeur", style="green")
            
            for key, value in self.user_config.items():
                table.add_row(key, str(value))
            
            console.print(table)
    
    def get_user_preferences(self) -> Dict[str, Any]:
        """Get user preferences for command-line arguments."""
        prefs = {}
        
        # Apply saved configuration
        if "default_target_language" in self.user_config:
            prefs["target_lang"] = self.user_config["default_target_language"]
        
        if "default_quality" in self.user_config:
            prefs["quality"] = self.user_config["default_quality"]
        
        if "gpu_memory_limit" in self.user_config:
            prefs["gpu_memory_limit"] = self.user_config["gpu_memory_limit"]
        
        if "batch_size" in self.user_config:
            prefs["batch_size"] = self.user_config["batch_size"]
        
        if "max_workers" in self.user_config:
            prefs["max_workers"] = self.user_config["max_workers"]
        
        if "enable_monitoring" in self.user_config:
            prefs["monitor"] = self.user_config["enable_monitoring"]
        
        if "log_level" in self.user_config:
            prefs["log_level"] = self.user_config["log_level"]
        
        return prefs


def create_enhanced_cli() -> InteractiveCLI:
    """Create enhanced CLI instance."""
    return InteractiveCLI()


if __name__ == "__main__":
    # Demo CLI
    cli = InteractiveCLI()
    cli.display_welcome_banner()
    
    mode = cli.show_quick_start_menu()
    
    if mode == "diagnostic":
        cli.run_system_diagnostic()
    elif mode == "tutorial":
        cli.show_tutorial()
    elif mode == "config":
        cli.configure_system_interactive()
    else:
        console.print(f"Mode s√©lectionn√©: {mode}")
        console.print("Int√©gration avec main.py en cours de d√©veloppement...")